import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

import os
from hydra.core.hydra_config import HydraConfig
from tqdm import tqdm
import wandb
os.environ["WANDB__SERVICE_WAIT"] = "300"

import torch
from torch.utils.data import DataLoader
import torch.distributed as dist

from utils.init_setup import dist_init, setup_model
import logging
from utils import logging_config

from dataloaders.random_dataset import StreamDataset

import hydra
from omegaconf import DictConfig


@torch.no_grad()
def inference(cfg, process_dict, run_dir):
    """Stream-only inference. Requires cfg.eval.stream_url or cfg.eval.url."""
    # Ensure cfg.inference is True so setup_model follows the inference/loader-free branch (same as train.py --inference)
    try:
        cfg.inference = True
    except Exception:
        pass

    model, _, _, train_step = setup_model(cfg, process_dict)
    model.eval()
    logging.info(f"Inference from step {train_step}")

    if getattr(cfg.eval, 'compile', False):
        model = torch.compile(model)

    # Only use run_dir generated by setup as the output directory for all results
    os.makedirs(run_dir, exist_ok=True)
    print('[inference run_dir]:{}'.format(run_dir))

    # Build eval_args, all outputs use run_dir
    eval_args = {
        'save_depth_npy': cfg.eval.save_depth_npy,
        'save_vis_map': cfg.eval.save_vis_map,
        'out_video': cfg.eval.out_video,
        'out_mp4': cfg.eval.out_mp4,
        'use_mamba': cfg.model.use_mamba,
        'resolution': cfg.eval.save_res,
        'print_time': True,
        'loss_type': cfg.training.loss_type,
        'use_all_frames': True,
        'use_metrics': False,
        'dummy_timing': cfg.eval.dummy_timing,
        'run_dir': run_dir
    }

    # Resolve stream address
    stream_addr = getattr(cfg.eval, 'stream_url', None) or getattr(cfg.eval, 'url', None)
    if stream_addr is None:
        raise ValueError('Stream address required: set cfg.eval.stream_url or cfg.eval.url')

    warmup = getattr(cfg.eval, 'stream_warmup_frames', 5)
    # If stream_max_frames is not provided, run indefinitely (no limit).
    _val = getattr(cfg.eval, 'stream_max_frames', None)
    if _val is None:
        max_frames = None
    else:
        try:
            max_frames = int(_val)
            # non-positive values -> no limit
            if max_frames <= 0:
                max_frames = None
        except Exception:
            logging.warning(f"Invalid cfg.eval.stream_max_frames value: {_val!r}. Treating as unlimited.")
            max_frames = None
    logging.info(f"Stream max frames set to: {max_frames}")

    logging.info('Using URL input: {}'.format(stream_addr))
    dataset = StreamDataset(stream_url=stream_addr, resolution=cfg.dataset.resolution, warmup_frames=warmup)
    test_dataloader = DataLoader(dataset, batch_size=1, num_workers=0, shuffle=False, drop_last=False)

    # Add frame rate control to prevent processing overload
    import time
    last_frame_time = 0
    target_fps = getattr(cfg.eval, 'target_fps', 30)  # Default 30 FPS
    frame_interval = 1.0 / target_fps if target_fps > 0 else 0

    # Use tqdm with a sensible total: if max_frames provided, use it as progress total
    if max_frames is None:
        pbar = tqdm(test_dataloader)
    else:
        try:
            total_val = int(max_frames)
        except Exception:
            total_val = None
        pbar = tqdm(test_dataloader, total=total_val)

    for test_idx, batch in enumerate(pbar):
        # Frame rate control
        current_time = time.time()
        if frame_interval > 0 and current_time - last_frame_time < frame_interval:
            sleep_time = frame_interval - (current_time - last_frame_time)
            time.sleep(sleep_time)
        last_frame_time = time.time()

        # Optionally stop after max_frames
        if (max_frames is not None) and (test_idx >= int(max_frames)):
            logging.info(f'Reached max_frames {max_frames}, stopping')
            break

        # StreamDataset returns dict with keys 'batch' and 'scene_name'
        if isinstance(batch, dict):
            batch_tensor = batch['batch']
            save_subdir = run_dir
            os.makedirs(save_subdir, exist_ok=True)
        else:
            batch_tensor = batch
            save_subdir = run_dir

        # Log input shape/dtype and ensure tensor is on model device
        try:
            model_device = next(model.parameters()).device
        except StopIteration:
            model_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

        # NOTE: suppressed per-frame input logging to reduce console clutter in streaming
        # If you want to re-enable, uncomment the logging.info line below.
        # logging.info(f"Model device: {model_device}, input shape: {getattr(batch_tensor, 'shape', None)}, dtype: {getattr(batch_tensor, 'dtype', None)}, device: {getattr(batch_tensor, 'device', None)}")

        if isinstance(batch_tensor, torch.Tensor) and batch_tensor.device != model_device:
            batch_tensor = batch_tensor.to(model_device)

        try:
            with torch.cuda.amp.autocast(dtype=torch.bfloat16):
                _, img_grid = model(
                    batch_tensor,
                    gif_path=f'{save_subdir}/{os.path.basename(cfg.config_dir.rstrip("/"))}_{train_step}_{test_idx}.gif',
                    **eval_args
                )

            if cfg.eval.save_grid and img_grid is not None:
                img_grid.save(f'{save_subdir}/{os.path.basename(cfg.config_dir.rstrip("/"))}_{train_step}_{test_idx}.png')
        except Exception as e:
            logging.warning(f"Error processing frame {test_idx}: {e}")
            continue  # Skip this frame and continue with next
    try:
        pbar.close()
    except Exception:
        pass


@hydra.main(config_path=None, config_name="config", version_base="1.3")
def setup(cfg: DictConfig):
    # initialize distributed/process
    process_dict = dist_init()
    logging_config.configure_logging()

    # Only use run_dir generated by setup as the output directory for all results
    result_root = os.path.abspath(os.path.join(os.getcwd(), 'result'))
    os.makedirs(result_root, exist_ok=True)
    idx = 0
    while True:
        candidate = os.path.join(result_root, f'stream_{idx}')
        if not os.path.exists(candidate):
            os.makedirs(candidate)
            run_dir = candidate
            break
        idx += 1

    # Add file logger to run_dir so logs are saved to stream_{N}/run.log
    fh = logging.FileHandler(os.path.join(run_dir, 'run.log'))
    fh.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    fh.setFormatter(formatter)
    logging.getLogger().addHandler(fh)

    hydra_cfg = HydraConfig.get()
    cfg.config_dir = [path["path"] for path in hydra_cfg.runtime.config_sources if path["schema"] == "file"][0]

    inference(cfg, process_dict, run_dir)

    dist.destroy_process_group()


if __name__ == '__main__':
    setup()
